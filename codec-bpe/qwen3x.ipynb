{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5735441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/potsawee/envs/env05/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nlp/scr/potsawee/envs/env05/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "marin_tokenizer = AutoTokenizer.from_pretrained(\"stanford-crfm/marin-tokenizer\")\n",
    "qwen3_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddff0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: ['tokenizer_config.json', 'added_tokens.json', 'tokenizer.json', 'chat_template.jinja', 'vocab.json', 'special_tokens_map.json', 'merges.txt']\n",
      "[vocab] '<|endoftext|>' not found in vocab\n",
      "[added_tokens] Swapped '<|endoftext|>' to '<|end_of_text|>'\n",
      "[special_tokens_map] Before: {'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>'], 'eos_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': False, 'rstrip': False, 'single_word': False}, 'pad_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': False, 'rstrip': False, 'single_word': False}}\n",
      "[special_tokens_map] Swapped pad_token to '<|end_of_text|>'\n",
      "[tokenizer_config] eos_token before: <|endoftext|>\n",
      "[tokenizer_config] pad_token before: <|endoftext|>\n",
      "[tokenizer_config] Swapped pad_token to '<|end_of_text|>'\n",
      "EOS token: <|end_of_text|>\n",
      "EOS token ID: 151643\n"
     ]
    }
   ],
   "source": [
    "# Swap existing EOS token from \"<|endoftext|>\" to \"<|end_of_text|>\" (keeping same ID 151643)\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Save tokenizer to temp directory\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    qwen3_tokenizer.save_pretrained(tmpdir)\n",
    "    \n",
    "    # Debug: list saved files\n",
    "    print(\"Saved files:\", os.listdir(tmpdir))\n",
    "    \n",
    "    # 1. Modify tokenizer.json\n",
    "    tokenizer_json_path = os.path.join(tmpdir, \"tokenizer.json\")\n",
    "    with open(tokenizer_json_path, \"r\") as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    # Replace in vocab\n",
    "    vocab = tokenizer_data[\"model\"][\"vocab\"]\n",
    "    if \"<|endoftext|>\" in vocab:\n",
    "        token_id = vocab.pop(\"<|endoftext|>\")\n",
    "        vocab[\"<|end_of_text|>\"] = token_id\n",
    "        print(f\"[vocab] Swapped '<|endoftext|>' to '<|end_of_text|>' with ID {token_id}\")\n",
    "    else:\n",
    "        print(\"[vocab] '<|endoftext|>' not found in vocab\")\n",
    "    \n",
    "    # Replace in added_tokens\n",
    "    if \"added_tokens\" in tokenizer_data:\n",
    "        for token_entry in tokenizer_data[\"added_tokens\"]:\n",
    "            if token_entry.get(\"content\") == \"<|endoftext|>\":\n",
    "                token_entry[\"content\"] = \"<|end_of_text|>\"\n",
    "                print(f\"[added_tokens] Swapped '<|endoftext|>' to '<|end_of_text|>'\")\n",
    "    \n",
    "    with open(tokenizer_json_path, \"w\") as f:\n",
    "        json.dump(tokenizer_data, f, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Modify special_tokens_map.json (eos_token and pad_token)\n",
    "    special_tokens_path = os.path.join(tmpdir, \"special_tokens_map.json\")\n",
    "    with open(special_tokens_path, \"r\") as f:\n",
    "        special_tokens = json.load(f)\n",
    "    print(f\"[special_tokens_map] Before: {special_tokens}\")\n",
    "    # Swap eos_token\n",
    "    if special_tokens.get(\"eos_token\") == \"<|endoftext|>\":\n",
    "        special_tokens[\"eos_token\"] = \"<|end_of_text|>\"\n",
    "    elif isinstance(special_tokens.get(\"eos_token\"), dict):\n",
    "        if special_tokens[\"eos_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            special_tokens[\"eos_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "    # Swap pad_token\n",
    "    if special_tokens.get(\"pad_token\") == \"<|endoftext|>\":\n",
    "        special_tokens[\"pad_token\"] = \"<|end_of_text|>\"\n",
    "    elif isinstance(special_tokens.get(\"pad_token\"), dict):\n",
    "        if special_tokens[\"pad_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            special_tokens[\"pad_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "            print(\"[special_tokens_map] Swapped pad_token to '<|end_of_text|>'\")\n",
    "    with open(special_tokens_path, \"w\") as f:\n",
    "        json.dump(special_tokens, f, ensure_ascii=False)\n",
    "    \n",
    "    # 3. Modify tokenizer_config.json (eos_token and pad_token)\n",
    "    tokenizer_config_path = os.path.join(tmpdir, \"tokenizer_config.json\")\n",
    "    with open(tokenizer_config_path, \"r\") as f:\n",
    "        tokenizer_config = json.load(f)\n",
    "    print(f\"[tokenizer_config] eos_token before: {tokenizer_config.get('eos_token')}\")\n",
    "    print(f\"[tokenizer_config] pad_token before: {tokenizer_config.get('pad_token')}\")\n",
    "    # Swap eos_token\n",
    "    if tokenizer_config.get(\"eos_token\") == \"<|endoftext|>\":\n",
    "        tokenizer_config[\"eos_token\"] = \"<|end_of_text|>\"\n",
    "    elif isinstance(tokenizer_config.get(\"eos_token\"), dict):\n",
    "        if tokenizer_config[\"eos_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            tokenizer_config[\"eos_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "    # Swap pad_token\n",
    "    if tokenizer_config.get(\"pad_token\") == \"<|endoftext|>\":\n",
    "        tokenizer_config[\"pad_token\"] = \"<|end_of_text|>\"\n",
    "        print(\"[tokenizer_config] Swapped pad_token to '<|end_of_text|>'\")\n",
    "    elif isinstance(tokenizer_config.get(\"pad_token\"), dict):\n",
    "        if tokenizer_config[\"pad_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            tokenizer_config[\"pad_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "            print(\"[tokenizer_config] Swapped pad_token to '<|end_of_text|>'\")\n",
    "    with open(tokenizer_config_path, \"w\") as f:\n",
    "        json.dump(tokenizer_config, f, ensure_ascii=False)\n",
    "    \n",
    "    # Reload tokenizer\n",
    "    qwen3_tokenizer = AutoTokenizer.from_pretrained(tmpdir)\n",
    "\n",
    "print(\"EOS token:\", qwen3_tokenizer.eos_token)\n",
    "print(\"EOS token ID:\", qwen3_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a13981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: <|begin_of_text|>\n",
      "BOS token ID: 151670\n"
     ]
    }
   ],
   "source": [
    "# Add bos_token to qwen3 tokenizer\n",
    "qwen3_tokenizer.add_special_tokens({\"bos_token\": \"<|begin_of_text|>\"})\n",
    "print(\"BOS token:\", qwen3_tokenizer.bos_token)\n",
    "print(\"BOS token ID:\", qwen3_tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a7671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: <|begin_of_text|>\n",
      "EOS token: <|end_of_text|>\n",
      "--------------------------------\n",
      "BOS token: <|begin_of_text|>\n",
      "EOS token: <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(\"BOS token:\", qwen3_tokenizer.bos_token)\n",
    "print(\"EOS token:\", qwen3_tokenizer.eos_token)\n",
    "print(\"--------------------------------\")\n",
    "print(\"BOS token:\", marin_tokenizer.bos_token)\n",
    "print(\"EOS token:\", marin_tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ce253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<|begin_of_text|>[2]Hello how are you<|end_of_text|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25316056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <|begin_of_text|>, token_id: 151670\n",
      "1: [, token_id: 58\n",
      "2: 2, token_id: 17\n",
      "3: ], token_id: 60\n",
      "4: Hello, token_id: 9707\n",
      "5:  how, token_id: 1246\n",
      "6:  are, token_id: 525\n",
      "7:  you, token_id: 498\n",
      "8: <|end_of_text|>, token_id: 151643\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(qwen3_tokenizer(text)['input_ids']):\n",
    "    print(f\"{i}: {qwen3_tokenizer.decode([token])}, token_id: {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23fe6c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <|begin_of_text|>\n",
      "1: <|begin_of_text|>\n",
      "2: [\n",
      "3: 3\n",
      "4: ]\n",
      "5: Hello\n",
      "6:  how\n",
      "7:  are\n",
      "8:  you\n",
      "9: <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(marin_tokenizer(text)['input_ids']):\n",
    "    print(f\"{i}: {marin_tokenizer.decode([token])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "595816fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload modified qwen3 tokenizer to Hugging Face\n",
    "# repo_name = \"potsawee/qwen3x-tokenizer\"  # Change this to your desired repo name\n",
    "# qwen3_tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda1e6b",
   "metadata": {},
   "source": [
    "## Build qwen3x-mimi model\n",
    "- Qwen3x = Qwen3 + vocab expanded by Mimi 8 codebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910064c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/QwenLM/Qwen3/issues/29\n",
    "# Tokenizer size and embedding size mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e0bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/potsawee/envs/env05/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nlp/scr/potsawee/envs/env05/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cb62a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = \"1.7B\" # 0.6B, 1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6bab0f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 1  : 151643\n",
      "vocab size 2  : 151669\n",
      "embedding size: 151936\n"
     ]
    }
   ],
   "source": [
    "model_name = f\"Qwen/Qwen3-{model_size}-Base\"\n",
    "qwen3_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "qwen3_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"vocab size 1  :\", qwen3_tokenizer.vocab_size)\n",
    "print(\"vocab size 2  :\", len(qwen3_tokenizer))\n",
    "print(\"embedding size:\", qwen3_model.model.embed_tokens.weight.shape[0])\n",
    "# The first one refers to the vocab size without special tokens, while the second includes them. The embedding size is larger with padding tokens due to distributed training in our pretraining stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e1ae21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 1 : 151643\n",
      "vocab size 2 : 168059\n"
     ]
    }
   ],
   "source": [
    "qwen3x_tokenizer = AutoTokenizer.from_pretrained(\"potsawee/qwen3x-mimi-bpe-8cb-16k-tokenizer\")\n",
    "print(\"vocab size 1 :\", qwen3x_tokenizer.vocab_size)\n",
    "print(\"vocab size 2 :\", len(qwen3x_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03beb561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168059\n"
     ]
    }
   ],
   "source": [
    "# 151669 -- original vocab size\n",
    "# 2048*8 -- vocab size from 8 Mimi codebooks\n",
    "# 4 -- <|audio_start|> <|audio_end|> <|text_start|> <|text_end|>\n",
    "# 2 -- <|begin_of_text|> <|end_of_text|>\n",
    "print(151669 + 2048*8 + 4 + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fca132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so I need the embedding size of qwen3x model to be at least 168059\n",
    "# Marin will handle padding automatically -- confirmed with the first \"ws\" experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "953ed67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the last cell, I want you the extend the current embedding of qwen3_model.model.embed_tokens.weight from 151936 to 168059\n",
    "\n",
    "# And for the expanded embedding, the positions [0,1,...,151668] should have the values copied over from the original embedding, while the positions [151669, 151670, 151671, ..., 168058] should be freshly initialized (i.e., from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18559636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original embedding shape: torch.Size([151936, 2048])\n",
      "Original embedding dtype: torch.bfloat16\n",
      "New embedding shape: torch.Size([168059, 2048])\n",
      "New embedding dtype: torch.bfloat16\n",
      "Mean embedding shape: torch.Size([2048])\n",
      "\n",
      "--- Summary ---\n",
      "New embedding size: 168059\n",
      "New embedding dtype: torch.bfloat16\n",
      "New lm_head size: 168059\n",
      "New lm_head dtype: torch.bfloat16\n",
      "Copied 151669 embeddings from original (positions 0 to 151668)\n",
      "Initialized 16390 new embeddings (positions 151669 to 168058) with mean + Gaussian noise (std=0.02)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Get original embedding\n",
    "original_embed = qwen3_model.model.embed_tokens.weight\n",
    "print(f\"Original embedding shape: {original_embed.shape}\")\n",
    "print(f\"Original embedding dtype: {original_embed.dtype}\")\n",
    "\n",
    "# Original dimensions\n",
    "original_vocab_size = original_embed.shape[0]  # 151936\n",
    "hidden_dim = original_embed.shape[1]\n",
    "dtype = original_embed.dtype  # bfloat16\n",
    "\n",
    "# New dimensions\n",
    "new_vocab_size = 168059\n",
    "num_tokens_to_copy = 151669  # Copy positions [0, 1, ..., 151668]\n",
    "num_new_tokens = new_vocab_size - num_tokens_to_copy\n",
    "\n",
    "# Create new embedding layer in bfloat16\n",
    "new_embed = nn.Embedding(new_vocab_size, hidden_dim, dtype=dtype)\n",
    "print(f\"New embedding shape: {new_embed.weight.shape}\")\n",
    "print(f\"New embedding dtype: {new_embed.weight.dtype}\")\n",
    "\n",
    "# Compute mean embedding from positions [0, ..., 151668]\n",
    "mean_embedding = original_embed[:num_tokens_to_copy].mean(dim=0)\n",
    "print(f\"Mean embedding shape: {mean_embedding.shape}\")\n",
    "\n",
    "# Small Gaussian noise std (you can adjust this)\n",
    "noise_std = 0.02\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Copy over the first 151669 embeddings from original\n",
    "    new_embed.weight[:num_tokens_to_copy] = original_embed[:num_tokens_to_copy].clone()\n",
    "    \n",
    "    # Initialize new positions with mean + small Gaussian noise (in bfloat16)\n",
    "    noise = (torch.randn(num_new_tokens, hidden_dim) * noise_std).to(dtype)\n",
    "    new_embed.weight[num_tokens_to_copy:] = mean_embedding.unsqueeze(0) + noise\n",
    "\n",
    "# Replace the embedding layer in the model\n",
    "qwen3_model.model.embed_tokens = new_embed\n",
    "\n",
    "# Also update lm_head to match the new vocabulary size\n",
    "original_lm_head = qwen3_model.lm_head.weight\n",
    "mean_lm_head = original_lm_head[:num_tokens_to_copy].mean(dim=0)\n",
    "\n",
    "new_lm_head = nn.Linear(hidden_dim, new_vocab_size, bias=False, dtype=dtype)\n",
    "with torch.no_grad():\n",
    "    new_lm_head.weight[:num_tokens_to_copy] = original_lm_head[:num_tokens_to_copy].clone()\n",
    "    noise = (torch.randn(num_new_tokens, hidden_dim) * noise_std).to(dtype)\n",
    "    new_lm_head.weight[num_tokens_to_copy:] = mean_lm_head.unsqueeze(0) + noise\n",
    "qwen3_model.lm_head = new_lm_head\n",
    "\n",
    "# Update model config\n",
    "qwen3_model.config.vocab_size = new_vocab_size\n",
    "\n",
    "print(f\"\\n--- Summary ---\")\n",
    "print(f\"New embedding size: {qwen3_model.model.embed_tokens.weight.shape[0]}\")\n",
    "print(f\"New embedding dtype: {qwen3_model.model.embed_tokens.weight.dtype}\")\n",
    "print(f\"New lm_head size: {qwen3_model.lm_head.weight.shape[0]}\")\n",
    "print(f\"New lm_head dtype: {qwen3_model.lm_head.weight.dtype}\")\n",
    "print(f\"Copied {num_tokens_to_copy} embeddings from original (positions 0 to {num_tokens_to_copy - 1})\")\n",
    "print(f\"Initialized {num_new_tokens} new embeddings (positions {num_tokens_to_copy} to {new_vocab_size - 1}) with mean + Gaussian noise (std={noise_std})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "499bc4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 4.20GB / 4.20GB, 95.0MB/s  \n",
      "New Data Upload: 100%|██████████|  135MB /  135MB, 70.6kB/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/potsawee/qwen3x-1.7B-base/commit/14cd3a126af7dfa8d4b518be4d9c4be439ac3934', commit_message='Upload Qwen3ForCausalLM', commit_description='', oid='14cd3a126af7dfa8d4b518be4d9c4be439ac3934', pr_url=None, repo_url=RepoUrl('https://huggingface.co/potsawee/qwen3x-1.7B-base', endpoint='https://huggingface.co', repo_type='model', repo_id='potsawee/qwen3x-1.7B-base'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen3_model.push_to_hub(f\"potsawee/qwen3x-{model_size}-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f114f507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2097779712"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of parameters in the model\n",
    "# 0.6B --> 784652288\n",
    "# 1.7B --> 2097779712\n",
    "qwen3_model.num_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
