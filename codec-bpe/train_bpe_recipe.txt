# Step1: load tokenized mimi codes from HF
python sample_yodas2_codes_from_hf.py  
    --shard-ids-file ../yodas2-mimi/shard_ids.txt \
    --subshard-counts-file ../yodas2-mimi/subshard_counts.json \
    --max-subshards-per-shard 5 \
    --repo-id potsawee/yodas2-mm \
    --output-dir ./yodas2_mimi_8cb_samples_organized \
    --subshard-list-file ./subset_subshard_ids_for_bpe.txt \
    --progress-file ./sample_progress.txt \
    --seed 42 \
    --num-target-codebooks 8

# Step2: train a new BPE tokenizer (Yodas2-Mimi)
## Step2.1: copy `bpe_trainer.py` to replace your codec_bpe package code: <path_to_your_env>/codec_bpe/core/trainer.py

## Step2.2: train a new BPE tokenizer
# 8 codebooks * 2048 = 16384
python -m codec_bpe.train_tokenizer \
    --codes_path yodas2_mimi_8cb_samples \
    --chunk_size_secs 30 \
    --vocab_size 128000 \
    --pad_token "<pad>" \
    --num_codebooks 8 \
    --codebook_size 2048 \
    --codec_framerate 12.5 \
    --max_token_codebook_ngrams 2 \
    --unicode_offset 0xE000 \
    --save_path ./trained_bpe_tokenizers/mimi_bpe_8cb_128k


# Step3: extend an existing Transformers PreTrainedTokenizer
python -m codec_bpe.extend_tokenizer \
    --existing_tokenizer stanford-crfm/marin-tokenizer \
    --codec_bpe_tokenizer ./trained_bpe_tokenizers/mimi_bpe_8cb_128k \
    --additional_special_tokens "<audio>" "</audio>" \
    --save_path ./trained_bpe_tokenizers/marin_mimi_bpe_8cb_128k


# ------------------------------------------------------------------------------------------------
# Build 8-codebook 16K BPE tokenizer (i.e., no merges)
python -m codec_bpe.train_tokenizer \
    --codes_path yodas2_mimi_8cb_samples_dummy3 \
    --chunk_size_secs 30 \
    --vocab_size 16384 \
    --pad_token "<pad>" \
    --num_codebooks 8 \
    --codebook_size 2048 \
    --codec_framerate 12.5 \
    --max_token_codebook_ngrams 0 \
    --unicode_offset 0xE000 \
    --save_path ./trained_bpe_tokenizers/mimi_bpe_8cb_16k

python -m codec_bpe.extend_tokenizer \
    --existing_tokenizer stanford-crfm/marin-tokenizer \
    --codec_bpe_tokenizer ./trained_bpe_tokenizers/mimi_bpe_8cb_16k \
    --additional_special_tokens "<|text_start|>" "<|text_end|>" "<|audio_start|>" "<|audio_end|>" \
    --save_path ./trained_bpe_tokenizers/marin_mimi_bpe_8cb_16k

# upload to HF
huggingface-cli login
huggingface-cli upload potsawee/marin-mimi-bpe-8cb-16k-tokenizer \
    /nlp/scr/potsawee/workspace/tokenize-audio/codec-bpe/trained_bpe_tokenizers/marin_mimi_bpe_8cb_16k \
    --repo-type model

# For extending Qwen3 tokenizer
python -m codec_bpe.extend_tokenizer \
    --existing_tokenizer Qwen/Qwen3-0.6B-Base \
    --codec_bpe_tokenizer ./trained_bpe_tokenizers/mimi_bpe_8cb_16k \
    --additional_special_tokens "<|text_start|>" "<|text_end|>" "<|audio_start|>" "<|audio_end|>" \
    --save_path ./trained_bpe_tokenizers/qwen3_mimi_bpe_8cb_16k

huggingface-cli upload potsawee/qwen3-mimi-bpe-8cb-16k-tokenizer \
    /nlp/scr/potsawee/workspace/tokenize-audio/codec-bpe/trained_bpe_tokenizers/qwen3_mimi_bpe_8cb_16k \
    --repo-type model

# For extending Qwen3x tokenizer
## Qwen3x tokenizer = Qwen3 tokenizer + <|begin_of_text|> and swapping <|endoftext|> to <|end_of_text|> to match Marin (Llama3) tokenizer
python -m codec_bpe.extend_tokenizer \
    --existing_tokenizer potsawee/qwen3x-tokenizer \
    --codec_bpe_tokenizer ./trained_bpe_tokenizers/mimi_bpe_8cb_16k \
    --additional_special_tokens "<|text_start|>" "<|text_end|>" "<|audio_start|>" "<|audio_end|>" \
    --save_path ./trained_bpe_tokenizers/qwen3x_mimi_bpe_8cb_16k

huggingface-cli upload potsawee/qwen3x-mimi-bpe-8cb-16k-tokenizer \
    /nlp/scr/potsawee/workspace/tokenize-audio/codec-bpe/trained_bpe_tokenizers/qwen3x_mimi_bpe_8cb_16k \
    --repo-type model