{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5735441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/potsawee/envs/env05/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nlp/scr/potsawee/envs/env05/lib/python3.10/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "marin_tokenizer = AutoTokenizer.from_pretrained(\"stanford-crfm/marin-tokenizer\")\n",
    "qwen3_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddff0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: ['special_tokens_map.json', 'vocab.json', 'merges.txt', 'chat_template.jinja', 'tokenizer.json', 'tokenizer_config.json', 'added_tokens.json']\n",
      "[vocab] '<|endoftext|>' not found in vocab\n",
      "[added_tokens] Swapped '<|endoftext|>' to '<|end_of_text|>'\n",
      "[special_tokens_map] Before: {'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>'], 'eos_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': False, 'rstrip': False, 'single_word': False}, 'pad_token': {'content': '<|endoftext|>', 'lstrip': False, 'normalized': False, 'rstrip': False, 'single_word': False}}\n",
      "[special_tokens_map] Swapped pad_token to '<|end_of_text|>'\n",
      "[tokenizer_config] eos_token before: <|endoftext|>\n",
      "[tokenizer_config] pad_token before: <|endoftext|>\n",
      "[tokenizer_config] Swapped pad_token to '<|end_of_text|>'\n",
      "EOS token: <|end_of_text|>\n",
      "EOS token ID: 151643\n"
     ]
    }
   ],
   "source": [
    "# Swap existing EOS token from \"<|endoftext|>\" to \"<|end_of_text|>\" (keeping same ID 151643)\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Save tokenizer to temp directory\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    qwen3_tokenizer.save_pretrained(tmpdir)\n",
    "    \n",
    "    # Debug: list saved files\n",
    "    print(\"Saved files:\", os.listdir(tmpdir))\n",
    "    \n",
    "    # 1. Modify tokenizer.json\n",
    "    tokenizer_json_path = os.path.join(tmpdir, \"tokenizer.json\")\n",
    "    with open(tokenizer_json_path, \"r\") as f:\n",
    "        tokenizer_data = json.load(f)\n",
    "    \n",
    "    # Replace in vocab\n",
    "    vocab = tokenizer_data[\"model\"][\"vocab\"]\n",
    "    if \"<|endoftext|>\" in vocab:\n",
    "        token_id = vocab.pop(\"<|endoftext|>\")\n",
    "        vocab[\"<|end_of_text|>\"] = token_id\n",
    "        print(f\"[vocab] Swapped '<|endoftext|>' to '<|end_of_text|>' with ID {token_id}\")\n",
    "    else:\n",
    "        print(\"[vocab] '<|endoftext|>' not found in vocab\")\n",
    "    \n",
    "    # Replace in added_tokens\n",
    "    if \"added_tokens\" in tokenizer_data:\n",
    "        for token_entry in tokenizer_data[\"added_tokens\"]:\n",
    "            if token_entry.get(\"content\") == \"<|endoftext|>\":\n",
    "                token_entry[\"content\"] = \"<|end_of_text|>\"\n",
    "                print(f\"[added_tokens] Swapped '<|endoftext|>' to '<|end_of_text|>'\")\n",
    "    \n",
    "    with open(tokenizer_json_path, \"w\") as f:\n",
    "        json.dump(tokenizer_data, f, ensure_ascii=False)\n",
    "    \n",
    "    # 2. Modify special_tokens_map.json (eos_token and pad_token)\n",
    "    special_tokens_path = os.path.join(tmpdir, \"special_tokens_map.json\")\n",
    "    with open(special_tokens_path, \"r\") as f:\n",
    "        special_tokens = json.load(f)\n",
    "    print(f\"[special_tokens_map] Before: {special_tokens}\")\n",
    "    # Swap eos_token\n",
    "    if special_tokens.get(\"eos_token\") == \"<|endoftext|>\":\n",
    "        special_tokens[\"eos_token\"] = \"<|end_of_text|>\"\n",
    "    elif isinstance(special_tokens.get(\"eos_token\"), dict):\n",
    "        if special_tokens[\"eos_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            special_tokens[\"eos_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "    # Swap pad_token\n",
    "    if special_tokens.get(\"pad_token\") == \"<|endoftext|>\":\n",
    "        special_tokens[\"pad_token\"] = \"<|end_of_text|>\"\n",
    "    elif isinstance(special_tokens.get(\"pad_token\"), dict):\n",
    "        if special_tokens[\"pad_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            special_tokens[\"pad_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "            print(\"[special_tokens_map] Swapped pad_token to '<|end_of_text|>'\")\n",
    "    with open(special_tokens_path, \"w\") as f:\n",
    "        json.dump(special_tokens, f, ensure_ascii=False)\n",
    "    \n",
    "    # 3. Modify tokenizer_config.json (eos_token and pad_token)\n",
    "    tokenizer_config_path = os.path.join(tmpdir, \"tokenizer_config.json\")\n",
    "    with open(tokenizer_config_path, \"r\") as f:\n",
    "        tokenizer_config = json.load(f)\n",
    "    print(f\"[tokenizer_config] eos_token before: {tokenizer_config.get('eos_token')}\")\n",
    "    print(f\"[tokenizer_config] pad_token before: {tokenizer_config.get('pad_token')}\")\n",
    "    # Swap eos_token\n",
    "    if tokenizer_config.get(\"eos_token\") == \"<|endoftext|>\":\n",
    "        tokenizer_config[\"eos_token\"] = \"<|end_of_text|>\"\n",
    "    elif isinstance(tokenizer_config.get(\"eos_token\"), dict):\n",
    "        if tokenizer_config[\"eos_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            tokenizer_config[\"eos_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "    # Swap pad_token\n",
    "    if tokenizer_config.get(\"pad_token\") == \"<|endoftext|>\":\n",
    "        tokenizer_config[\"pad_token\"] = \"<|end_of_text|>\"\n",
    "        print(\"[tokenizer_config] Swapped pad_token to '<|end_of_text|>'\")\n",
    "    elif isinstance(tokenizer_config.get(\"pad_token\"), dict):\n",
    "        if tokenizer_config[\"pad_token\"].get(\"content\") == \"<|endoftext|>\":\n",
    "            tokenizer_config[\"pad_token\"][\"content\"] = \"<|end_of_text|>\"\n",
    "            print(\"[tokenizer_config] Swapped pad_token to '<|end_of_text|>'\")\n",
    "    with open(tokenizer_config_path, \"w\") as f:\n",
    "        json.dump(tokenizer_config, f, ensure_ascii=False)\n",
    "    \n",
    "    # Reload tokenizer\n",
    "    qwen3_tokenizer = AutoTokenizer.from_pretrained(tmpdir)\n",
    "\n",
    "print(\"EOS token:\", qwen3_tokenizer.eos_token)\n",
    "print(\"EOS token ID:\", qwen3_tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a13981e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: <|begin_of_text|>\n",
      "BOS token ID: 151670\n"
     ]
    }
   ],
   "source": [
    "# Add bos_token to qwen3 tokenizer\n",
    "qwen3_tokenizer.add_special_tokens({\"bos_token\": \"<|begin_of_text|>\"})\n",
    "print(\"BOS token:\", qwen3_tokenizer.bos_token)\n",
    "print(\"BOS token ID:\", qwen3_tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a7671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: <|begin_of_text|>\n",
      "EOS token: <|end_of_text|>\n",
      "--------------------------------\n",
      "BOS token: <|begin_of_text|>\n",
      "EOS token: <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(\"BOS token:\", qwen3_tokenizer.bos_token)\n",
    "print(\"EOS token:\", qwen3_tokenizer.eos_token)\n",
    "print(\"--------------------------------\")\n",
    "print(\"BOS token:\", marin_tokenizer.bos_token)\n",
    "print(\"EOS token:\", marin_tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<|begin_of_text|>Hello how are you<|end_of_text|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25316056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <|begin_of_text|>, token_id: 151670\n",
      "1: Hello, token_id: 9707\n",
      "2:  how, token_id: 1246\n",
      "3:  are, token_id: 525\n",
      "4:  you, token_id: 498\n",
      "5: <|end_of_text|>, token_id: 151643\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(qwen3_tokenizer(text)['input_ids']):\n",
    "    print(f\"{i}: {qwen3_tokenizer.decode([token])}, token_id: {token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fe6c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <|begin_of_text|>\n",
      "1: <|begin_of_text|>\n",
      "2: Hello\n",
      "3:  how\n",
      "4:  are\n",
      "5:  you\n",
      "6: <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(marin_tokenizer(text)['input_ids']):\n",
    "    print(f\"{i}: {marin_tokenizer.decode([token])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595816fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 11.4MB / 11.4MB, 1.33MB/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/potsawee/qwen3x-tokenizer/commit/0edb09a3f9cbdde54ecee5132b8daa76c1ed1d9a', commit_message='Upload tokenizer', commit_description='', oid='0edb09a3f9cbdde54ecee5132b8daa76c1ed1d9a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/potsawee/qwen3x-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='potsawee/qwen3x-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload modified qwen3 tokenizer to Hugging Face\n",
    "repo_name = \"potsawee/qwen3x-tokenizer\"  # Change this to your desired repo name\n",
    "qwen3_tokenizer.push_to_hub(repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env05",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
